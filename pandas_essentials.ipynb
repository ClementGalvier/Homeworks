{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\clement.galvier\\OneDrive - VIRGIN ACTIVE SINGAPORE PTE LTD\\Documents\\GeneralAssembly\\Homeworks\\pandas_essentials.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Homeworks/pandas_essentials.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39m#check for nulls\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Homeworks/pandas_essentials.ipynb#ch0000000?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39msum()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Homeworks/pandas_essentials.ipynb#ch0000000?line=3'>4</a>\u001b[0m \u001b[39m#get unique values of a dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Homeworks/pandas_essentials.ipynb#ch0000000?line=4'>5</a>\u001b[0m bikedf_dummy[\u001b[39m'\u001b[39m\u001b[39mbirth year\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#check for nulls\n",
    "df.isnull().sum()\n",
    "\n",
    "#get unique values of a dataset\n",
    "bikedf_dummy['birth year'].unique()\n",
    "\n",
    "\n",
    "#replace wrong data\n",
    "#option1\n",
    "df['Age'] = [np.nan if age=='?' else float(age) for age in df['Age'].values]\n",
    "#option2\n",
    "bikedf_dummy['birth year'].replace(to_replace='\\\\N',value = 0,inplace=True)\n",
    "\n",
    "\n",
    "#transform a column type from object to int\n",
    "bikedf_dummy['birth year'] = bikedf_dummy['birth year'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#select df subset ( filter)\n",
    "Y = bikedf.loc[bikedf[\"gender\"] == 2, \"tripduration\"]\n",
    "\n",
    "\n",
    "# 1 - Rename \"count\" to \"riders\"\n",
    "# 2 - Drop \"casual\" and \"registered\"\n",
    "# 3 - Drop only rows where weather == 4\n",
    "# 4 - Cast \"datetime\" to actually be a datetime\n",
    "bike.rename(columns={\"count\": \"riders\"}, inplace=True) # solution 1\n",
    "bike.drop(columns=[\"casual\", \"registered\"], inplace=True) # solution 2\n",
    "bike = bike[bike[\"weather\"] != 4] # solution 3\n",
    "bike[\"datetime\"] = pd.to_datetime(bike[\"datetime\"]) # solution 4\n",
    "\n",
    "\n",
    "#replace missing values THE MANUAL WAY\n",
    "#replace missing data using mean\n",
    "mean_age = income_missing['age'].mean()\n",
    "income_missing['age_mean_imputed'] = income_missing['age'].fillna(mean_age)\n",
    "#then check the variance between before and after !\n",
    "#replace missing data using meadian\n",
    "median_age = income_missing['age'].median()\n",
    "income_missing['age_mean_imputed'] = income_missing['age'].fillna(median)\n",
    "#replace missing data using mode ( most frequent )\n",
    "mode_age = income_missing['age'].mode()\n",
    "income_missing['age_mean_imputed'] = income_missing['age'].fillna(mode)\n",
    "\n",
    "\n",
    "#replace missing values THE SKLEARN WAY\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "#Charts\n",
    "\n",
    "#heatmap\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm');\n",
    "\n",
    "\n",
    "#heatmap>|0.5| only\n",
    "pearson_corr_features = df.corr()\n",
    "\n",
    "corr_data = np.tril(pearson_corr_features,k=-1) # only looking at bottom heatmap triangle\n",
    "corr_data = abs(corr_data)>0.50 # only visualizing notable correlations \n",
    "\n",
    "# plotting\n",
    "fig = px.imshow(corr_data, x=pearson_corr_features.columns, y=pearson_corr_features.index,\n",
    "                color_continuous_scale=px.colors.qualitative.Plotly, width=800, height=800,\n",
    "                title=\"Heat map: Feature correlations with >0.5 correlation coefficient\")\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a04f5bcc4cce45f88f55048bde1d1871ccb10dc4647418ede9299c65043aaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
