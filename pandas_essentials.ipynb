{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\clement.galvier\\OneDrive - VIRGIN ACTIVE SINGAPORE PTE LTD\\Documents\\GeneralAssembly\\Homeworks\\pandas_essentials.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Homeworks/pandas_essentials.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39m#check for nulls\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Homeworks/pandas_essentials.ipynb#ch0000000?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39msum()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Homeworks/pandas_essentials.ipynb#ch0000000?line=3'>4</a>\u001b[0m \u001b[39m#get unique values of a dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Homeworks/pandas_essentials.ipynb#ch0000000?line=4'>5</a>\u001b[0m bikedf_dummy[\u001b[39m'\u001b[39m\u001b[39mbirth year\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#check for nulls\n",
    "df.isnull().sum()\n",
    "\n",
    "#get unique values of a dataset\n",
    "bikedf_dummy['birth year'].unique()\n",
    "\n",
    "\n",
    "#replace wrong data\n",
    "#option1\n",
    "df['Age'] = [np.nan if age=='?' else float(age) for age in df['Age'].values]\n",
    "#option2\n",
    "bikedf_dummy['birth year'].replace(to_replace='\\\\N',value = 0,inplace=True)\n",
    "\n",
    "\n",
    "#transform a column type from object to int\n",
    "bikedf_dummy['birth year'] = bikedf_dummy['birth year'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#select df subset ( filter)\n",
    "Y = bikedf.loc[bikedf[\"gender\"] == 2, \"tripduration\"]\n",
    "\n",
    "\n",
    "# 1 - Rename \"count\" to \"riders\"\n",
    "# 2 - Drop \"casual\" and \"registered\"\n",
    "# 3 - Drop only rows where weather == 4\n",
    "# 4 - Cast \"datetime\" to actually be a datetime\n",
    "bike.rename(columns={\"count\": \"riders\"}, inplace=True) # solution 1\n",
    "bike.drop(columns=[\"casual\", \"registered\"], inplace=True) # solution 2\n",
    "bike = bike[bike[\"weather\"] != 4] # solution 3\n",
    "bike[\"datetime\"] = pd.to_datetime(bike[\"datetime\"]) # solution 4\n",
    "\n",
    "\n",
    "#replace missing values THE MANUAL WAY\n",
    "#replace missing data using mean\n",
    "mean_age = income_missing['age'].mean()\n",
    "income_missing['age_mean_imputed'] = income_missing['age'].fillna(mean_age)\n",
    "#then check the variance between before and after !\n",
    "#replace missing data using meadian\n",
    "median_age = income_missing['age'].median()\n",
    "income_missing['age_mean_imputed'] = income_missing['age'].fillna(median)\n",
    "#replace missing data using mode ( most frequent )\n",
    "mode_age = income_missing['age'].mode()\n",
    "income_missing['age_mean_imputed'] = income_missing['age'].fillna(mode)\n",
    "\n",
    "\n",
    "#replace missing values THE SKLEARN WAY\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "#Charts\n",
    "\n",
    "#heatmap\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm');\n",
    "\n",
    "\n",
    "#heatmap>|0.5| only\n",
    "pearson_corr_features = df.corr()\n",
    "\n",
    "corr_data = np.tril(pearson_corr_features,k=-1) # only looking at bottom heatmap triangle\n",
    "corr_data = abs(corr_data)>0.50 # only visualizing notable correlations \n",
    "\n",
    "# plotting\n",
    "fig = px.imshow(corr_data, x=pearson_corr_features.columns, y=pearson_corr_features.index,\n",
    "                color_continuous_scale=px.colors.qualitative.Plotly, width=800, height=800,\n",
    "                title=\"Heat map: Feature correlations with >0.5 correlation coefficient\")\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values[1] = 'Courses_Fee'\n",
    "print(df.columns)\n",
    "\n",
    "# Rename column name by index\n",
    "df.rename(columns={df.columns[2]: 'Courses_Duration'},inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SUPERVISED MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# X_poly is a list-of-lists with each 9 transformed feature values\n",
    "# printing row 1 as sample preview to compare vs X pre-transformation\n",
    "print(f'pre-X: {X.iloc[1].values}')\n",
    "print(f'post-X: {X_poly[[1]]}') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ss = StandardScaler() # instantiate\n",
    "X_scaled = ss.fit_transform(X) # fit and transform (scale) X\n",
    "X_scaled[0, :] # preview post-scaling output from 0th row and all cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn import metrics \n",
    "lr = LinearRegression()\n",
    "model = LinearRegression()\n",
    "\n",
    "X = bike[[\"season\", \"temp\"]] # get a subset_df with all col+values from season, temp cols\n",
    "X = pd.get_dummies(data=X, columns=[\"season\"], drop_first=True) # dropping season 1\n",
    "y = bike[\"riders\"]\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "dict(zip(X.columns, model.coef_))\n",
    "\n",
    "# Make predictions using sklearn's predict() method\n",
    "y_pred = lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding cross validation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "\n",
    "cross_val_score(lr, X_train, y_train, cv=5)# defaults to linear regression's default scoring, R-squared\n",
    "cross_val_score(lr, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addting regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "r_alphas = np.logspace(0, 5, 100)\n",
    "\n",
    "# Cross-validate over our list of ridge alphas.\n",
    "# alphas: pass an Array of alpha values to try. It is still the Regularization strength\n",
    "ridge_cv = RidgeCV(alphas=r_alphas, scoring='r2', cv=5).fit(Z_train, y_train)# fitting 5-fold CV on 100 alphas, that is 500 alphas!\n",
    "print(ridge_cv.score(Z_train, y_train))\n",
    "print(ridge_cv.score(Z_test, y_test))\n",
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports similar to Ridge, this time for Lasso instead\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "# then converts them to alphas between 10^-3 and 1 (that is, in logscale).\n",
    "l_alphas = np.logspace(-3, 0, 100)\n",
    "\n",
    "# Cross-validate over our list of Lasso alphas.\n",
    "lasso_cv = LassoCV(alphas=l_alphas, cv=5, max_iter=50000).fit(Z_train, y_train);\n",
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_sc, y_train)\n",
    "\n",
    "# See the gap in training vs testing metric! Overfit!\n",
    "print(logreg.score(X_train_sc, y_train))\n",
    "print(logreg.score(X_test_sc, y_test))\n",
    "\n",
    "\n",
    "# we arrive at the same outcome by setting penalty='none'\n",
    "logreg_pen_none = LogisticRegression(penalty='none')\n",
    "logreg_pen_none.fit(X_train_sc, y_train)\n",
    "\n",
    "# See the gap in training vs testing metric! Overfit!\n",
    "print(logreg_pen_none.score(X_train_sc, y_train))\n",
    "print(logreg_pen_none.score(X_test_sc, y_test))\n",
    "\n",
    "##---------------------------\n",
    "logreg_cv = LogisticRegressionCV(cv=5, penalty=\"l1\", solver=\"liblinear\")\n",
    "logreg_cv.fit(X_train_sc, y_train)\n",
    "\n",
    "# we don't see overfitting now! training-testing metrics are close\n",
    "print(logreg_cv.score(X_train_sc, y_train))\n",
    "print(logreg_cv.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note on stats model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = elec[[\"temp\", \"workday\"]]\n",
    "X = sm.add_constant(X) # remember that unlike scikit-learn, statsmodels needs this to be explicitly specified\n",
    "y = elec[\"demand\"]\n",
    "ols = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "from math import sqrt # alternative to (variable)**0.5 ( imported from sklearn too)\n",
    "\n",
    "resids = y - y_pred\n",
    "mse = np.mean(resids**2)\n",
    "y_bar = np.mean(y)\n",
    "null_mse = np.mean((y - y_bar)**2)\n",
    "r_squared = mse/null_mse\n",
    "\n",
    "\n",
    "metrics.mean_squared_error(y, predictions)\n",
    "np.sqrt(metrics.mean_squared_error(y, predictions))\n",
    "metrics.mean_absolute_error(y, predictions)\n",
    "metrics.r2_score(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charting for linear regression\n",
    "sns.regplot(x='temp', y='demand', data=elec, ci=None, \n",
    "            scatter_kws = {'s': 1}, \n",
    "            line_kws = {'color': 'orange'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'body', y = 'brain', data = mammals,\n",
    "           hue = 'universe', ci = False, order = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a04f5bcc4cce45f88f55048bde1d1871ccb10dc4647418ede9299c65043aaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
